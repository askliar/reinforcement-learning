\documentclass{exam}
\input{head}
\usepackage{inputenc}
\title{Reinforcement Learning - Homework 1} \date{deadline: November 16, 2018}
\author{Andrii Skliar, 11636785\\ Gabriele Bani, 11636758}

\begin{document}
\maketitle 

% \fbox{
%   \parbox{0.8\textwidth}{
%      During the process of solving the homework problems, I have collaborated with the following colleagues: \\\\
%     % LIST ALL THE COLLABORATORS HERE!
%     \begin{tabular}{c c c c}
%         Gabriele Bani & Gabriele Cesa & Davide Belli & Pascal Esser\\
%         Gautier Dagan & & &
%         % more people? put them here:
%         %  &   &   
%   \end{tabular}\\\\
%   \textit{\small NB: credits for the Latex-format go to Iris Verweij, 2nd year MSc AI Student}.
%   }
% }

% \vspace{0.8cm}

%------------------------------
% Problem 1

\begin{problem}[Exploration]
\ \newline
\begin{enumerate}
    \item In $\epsilon$-greedy action-selection for the case of n actions, what is the probability of selecting the greedy action?
    \begin{solutionorlines}[2in]
        Probability of selecting the greedy action is $1 - \epsilon$.
    \end{solutionorlines}
    \item Consider a 3-armed bandit problem with actions 1, 2, 3. If we use $\epsilon$-greedy action-selection, initialization at 0, and sample-average action-value estimates, which of the following sequence of actions are certain to be the result of exploration? $A_1 = 1, R_1 = −1, A_2 = 2, R_2 = 1, A_3 = 2, R_3 = −2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 1$.
    \begin{solutionorlines}[2in]
        Actions $A_4, A_5$ are results of exploration. This can be seen by calculating average action-value estimates at each timestep and seeing when the action taken is not the one that yields best estimates.
    \end{solutionorlines}
    \item You are trying to find the optimal policy for a two-armed bandit. You try two approaches: in the pessimistic approach, you initialize all action-values at $-5$, and in the optimistic approach you initialize all action-values at $+5$. One arm gives a reward of $+1$, one arm gives a reward of $-1$. Using a greedy policy to choose actions, compute the resulting Q-values for both actions after three interactions with the environment. In case of a tie between two Q-values, break the tie at random.
    \begin{solutionorlines}[2in]
    We assume that arm $A_1$ gives reward of $+1$ and arm $A_2$ gives reward of $-1$.\\
    \begin{center}
    \def\arraystretch{1.5}
    \begin{tabular}{|c|c|c|l|}
    \hline
    Step & $Q_1 (+1)$ & $Q_2 (-1)$ & Action Taken \\ \hline
    0 & $-5$ & $-5$ & \begin{tabular}[c]{@{}l@{}}All action values are the same, select arm randomly. \\ Assume, $Q_1$ has been selected.\end{tabular} \\ \hline
    1 & $1$ & $-5$ & Greedily choose $Q_1$ \\ \hline
    2 & $1$ & $-5$ & Greedily choose $Q_1$ \\ \hline
    3 & $1$ & $-5$ & \multicolumn{1}{c|}{} \\ \hline
    \end{tabular}
    \end{center}
    \newpage
    
    \begin{center}
    \def\arraystretch{1.5}
    \begin{tabular}{|c|c|c|l|}
    \hline
    Step & $Q_1 (+1)$ & $Q_2 (-1)$ & Action Taken \\ \hline
    0 & $5$ & $5$ & \begin{tabular}[c]{@{}l@{}}All action values are the same, select arm randomly. \\ Assume, $Q_1$ has been selected.\end{tabular} \\ \hline
    1 & $1$ & $5$ & Greedily choose $Q_2$ \\ \hline
    2 & $1$ & $-1$ & Greedily choose $Q_1$ \\ \hline
    3 & $1$ & $-1$ & \multicolumn{1}{c|}{} \\ \hline
    \end{tabular}
    \end{center}
    
    \end{solutionorlines}
    
    \begin{centering}
    \begin{table}[]
    
    \end{table}
    \end{centering}
    
    \item Which initialization leads to a higher (undiscounted) return? What if you had broken the tie differently?
    \begin{solutionorlines}[2in]
        Pessimistic initialization leads to higher return because in this case it will lead to pure exploitation without any exploration. However, if we would break tie choosing $Q_2$ in the first step, we would never choose $Q_1$ thus getting only negative rewards. 
        Optimistic initialization, on the other hand, will lead to the same return even with any break in the first step (at least for the first 3 steps). Asymptotically, its performance should also be better due to the fact described in the next part of the question. 
    \end{solutionorlines}
    \item Which initialization leads to a better estimation of the Q-values?
    \begin{solutionorlines}[2in]
        Optimistic initialization leads to a better estimation of the Q-values as, unlike pessimistic initialization, it allows for exploration and not just pure exploitation strategy. Pessimistic initialization asymptotically can only lead to a good estimation of a Q-value of a single hand (the one chosen first), while optimistic initialization can asymptotically lead to a good estimation of all Q-values.
    \end{solutionorlines}
    
    \item Explain why one of the two initialization methods is better for exploration.
    \begin{solutionorlines}[2in]
        Optimistic initialization is better for exploration due to the fact that it makes initial Q-value larger than any possible action-value, so after taking any action, greedy policy forces us to use other, not yet explored actions as their action-value is higher. At the same time, pessimistic initialization will choose the same action at all timesteps. It is due to the fact that all the initial action-values are smaller than then action-value of an action chosen at the first timestep and will enforce that only this action will be chosen at any future timestep.
    \end{solutionorlines}
\end{enumerate}

\end{problem}

%--------------------------------------

\begin{problem}[Dynamic Programming]
\ \newline
\begin{enumerate}
    \item Write the value, $\nu^{\pi}(s)$, of a state $s$ under policy $\pi$, in terms of $\pi$ and $q^{\pi}(s,a)$. Write down both the stochastic and the deterministic policy case.
    \begin{solutionorlines}[2in]
    \begin{align*}
        \nu^{\pi}(s) &=\sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma \nu^{\pi}(s')]\\
        &= \sum_a \pi(a|s) q^{\pi}(s,a) \tag{stochastic case}\\
        &= q^{\pi}(s, a') \tag{deterministic case with action $a'$}
    \end{align*}
    \end{solutionorlines}
    \item In Policy Iteration, we first evaluate the policy by computing its value function, and then update it using a Policy Improvement step. You will now change Policy Iteration as given on page 80 of the book to compute action-values. First give the new policy evaluation update in terms of $Q^{\pi}(s, a)$ instead of $V^{\pi}(s)$. Note that Policy Improvement uses deterministic policies.
    \begin{solutionorlines}[2in]
    \ \\
    Policy Evaluation\\
    Loop:
    \begin{itemize}[noitemsep,nolistsep]
        \item[ ] $\Delta \leftarrow 0$
        \item[ ] Loop for each $s \in \mathcal{S}$:
        \begin{itemize}[noitemsep,nolistsep]
            \item[ ] Loop for each $a \in \mathcal{A}(s)$:
            \begin{itemize}[noitemsep,nolistsep]
                \item[ ] $q \leftarrow Q(s, a)$
                \item[ ] $Q(s, a) \leftarrow \sum_{s', r} p(s', r| s, a) [r + \gamma \sum_{a'} \pi(a'|s') Q^{\pi}(s', a')]$
                \item[ ] $\Delta \leftarrow max(\Delta, |q - Q(s, a)|)$
            \end{itemize}
        \end{itemize}
    \end{itemize}
    until $\Delta < \theta$
    \end{solutionorlines}
    \item Now change the Policy Improvement update in terms of $Q^{\pi}(s, a)$ instead of $V^{\pi}(s)$.
    \begin{solutionorlines}[2in]
    \ \\
    Policy Improvement\\
    \textit{policy-stable} $\leftarrow true$\\
    For each $s \in \mathcal{S}$:
    \begin{itemize}[noitemsep,nolistsep]
        \item[ ] \textit{old-action} $\leftarrow \pi(s)$
        \item[ ] $\pi(s) \leftarrow \argmax_a \sum_{s', r} p(s', r| s, a) [r + \gamma \sum_{a'} \pi(a'|s') Q^{\pi}(s', a')]$
        \item[ ] If \textit{old-action} $\neq \pi(s)$, then \textit{policy-stable} $\leftarrow false$
    \end{itemize}
    If \textit{policy-stable}, then stop and return $Q \approx q_*$ and $\pi \approx \pi_*$; else go to Policy Evaluation
    \end{solutionorlines}
    \item The Value Iteration update, given in the book in Eq. 4.10 can also be rewritten in terms of Q-values. Give the Q-value Iteration update.
    \begin{solutionorlines}[2in]
    Note that we are skipping initialization step. \\
    Loop:
    \begin{itemize}[noitemsep,nolistsep]
        \item[ ] $\Delta \leftarrow 0$
        \item[ ] Loop for each $s \in \mathcal{S}$:
        \begin{itemize}[noitemsep,nolistsep]
            \item[ ] Loop for each $a \in \mathcal{A}(s)$:
            \begin{itemize}[noitemsep,nolistsep]
                \item[ ] $q \leftarrow Q(s, a)$
                \item[ ] $Q(s, a) \leftarrow \sum_{s', r} p(s', r| s, a) [r + \gamma \sum_{a'} \pi(a'|s') Q^{\pi}(s', a')]$
                \item[ ] $\Delta \leftarrow max(\Delta, |q - Q(s, a)|)$
            \end{itemize}
        \end{itemize}
    \end{itemize}
    until $\Delta < \theta$\\
    \\
    Output a deterministic policy $\pi \approx \pi_*$, such that\\
    $\pi(s) \leftarrow \argmax_a \sum_{s', r} p(s', r| s, a) [r + \gamma \sum_{a'} \pi(a'|s') Q^{\pi}(s', a')]$
    \end{solutionorlines}
\end{enumerate}

\end{problem}

\begin{problem}[Temporal Difference Learning (Application)]
\ \newline
\begin{enumerate}
    \item 
    What are the state(-action) value estimates $V(s)$ (or $Q(s,a)$) after observing the sample episode when applying:
    
    \begin{enumerate}[(a)]
        \item TD(0)
        \begin{solutionorlines}[2in]
            \begin{align*}
                V(A) &= -0.893\\
                V(B) &= 0.433
            \end{align*}
        \end{solutionorlines}
        \item 3-step TD
        \begin{solutionorlines}[2in]
            \begin{align*}
                V(A) &= -0.983\\
                V(B) &= -0.3
            \end{align*}
        \end{solutionorlines}
        \item SARSA
        \begin{solutionorlines}[2in]
            \begin{align*}
                Q(A, 1) &= -0.57\\
                Q(A, 2) &= -0.43\\
                Q(B, 1) &= 0.4\\
                Q(B, 2) &= 0.1
            \end{align*}
        \end{solutionorlines}
        \item Q-learning
        \begin{solutionorlines}[2in]
            \begin{align*}
                Q(A, 1) &= -0.53\\
                Q(A, 2) &= -0.4\\
                Q(B, 1) &= 0.4\\
                Q(B, 2) &= 0.1
            \end{align*}
        \end{solutionorlines}
    \end{enumerate}
    \item Choose a deterministic policy that you think is better than the random policy given the data. Refer to any of the state(-action) value estimates to explain your reasoning.
    \begin{solutionorlines}[2in]
    Using the Q-values, learned in Q-learning algorithm (SARSA produces similar results, so we can use those as well), we can change our policy to be deterministic :
    \begin{align*}
        \pi(S=A) = 2\\
        \pi(S=B) = 1
    \end{align*}
    As we can see, such policy gives rise to the highest state-action value among all the possible options. 
    \end{solutionorlines}
    \item Let $\pi_{random}$ denote the random policy used so far and $\pi_{student}$ denote the new policy you proposed. Suppose you can draw new sample episodes indefinitely until convergence of the value estimates.
    \begin{enumerate}[(a)]
        \item Discuss how do you expect the final value estimates to differ if you ran Q-Learning with  $\pi_{random}$ as compared to $\pi_{student}$.
        \begin{solutionorlines}[2in]
        The main reason to use Q-learning is that it is an off-policy algorithm, which means that it uses one policy for exploring and the other one for exploiting the information provided by the exploration to update Q-values. The main consequence of that is that Q-learning convergence can only be proven for the case when \textbf{all} the state-action pairs are continuously updated, which is not the case if we use $\pi_{student}$, which is a greedy policy. Therefore, I would expect that $\pi_{random}$ would converge to real state-action values, while $\pi_{student}$ would not be able to converge to real values as most of the action-state pairs would never be explored enough to estimate anything meaningful. However, if we are interested in a short-term estimation, $\pi_{student}$ can potentially produce good estimation of the pairs $(A, 2)$ and $(B, 1)$ in a short run as they will be visited more often than if we would use $\pi_{random}$. However, this statement highly depends on the transition probabilities of the MDP.
        % As we can draw new sample episodes indefinitely until convergence of the value estimates, I Would expect $\pi_{random}$ to perform much better as, unlike $\pi_{student}$, it is not deterministic and therefore can provide sufficient exploration. This allows the final state-action values to get really close to the real state-action values. However, it might take much longer to converge than when using $\pi_{student}$, especially, if estimated state-action values are close to the real state-action values. The main reason not to use suggested policy, however, is that it eliminates the advantages of off-policy learning as our  
        \end{solutionorlines}
        \item What problems may arise with $\pi_{random}$ or $\pi_{student}$ respectively?
        \begin{solutionorlines}[2in]
        The main problem of $\pi_{student}$ has been discussed in the previous subquestion - as it is a greedy policy, it doesn't contribute to the exploration of the state-action values, which is a necessity for a good performance of Q-learning in a long run. 
        
        However, in a short term, it might not explore states, which potentially bring the highest expected reward, which is not the case for $\pi_{student}$, which will only explore those. However, $\pi_{student}$ can only provide the forementioned benefit in case the initial estimations were representative and system is stationary as in this case we can already expect the final state-action values not to change much compared to the estimation provided in the previous part of the question.
        \end{solutionorlines}
        \item Do you think using an $\epsilon$-greedy policy as behavior policy would be beneﬁcial? Explain why/why not?
        \begin{solutionorlines}[2in]
        Using $\epsilon$-greedy policy for the case of $\pi_{student}$ would be highly beneficial as it would eliminate the problem of not sufficient exploration that arises when using previously suggested policy. However, using it for the $\pi_{random}$ depends on the current values of the policy. Assuming that they are uniform (which is the most extreme case of randomness), it would be beneficial as it would allow us not only to explore all the states equally but also to explore states which are known to be good more. However, the closer $\pi_{random}$ is to epsilon-greedy policy, the less benefit changing the policy will give.
        \end{solutionorlines}
    \end{enumerate}
\end{enumerate}
\end{problem}

\bibliographystyle{plain}
\bibliography{bibliography}
\end{document}